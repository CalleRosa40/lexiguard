{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ChnD36qHhuy"
      },
      "source": [
        "# Custom predictions using fastText + XGBoost (Michael)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLTUtsD6HupB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nisbn3mTG0fj",
        "outputId": "c3a65a47-0bea-4de2-c594-1b5ca57dec48"
      },
      "outputs": [],
      "source": [
        "# import the usual suspects / basics\n",
        "import time; full_run_time_start = time.time() # start timing exec right away\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "from scipy import sparse\n",
        "import re\n",
        "import os\n",
        "\n",
        "# scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, classification_report, f1_score,\\\n",
        "    accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "# XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# fastText\n",
        "import fasttext\n",
        "\n",
        "# spaCy\n",
        "import spacy\n",
        "\n",
        "# currently not used and thus commented out\n",
        "# import nltk\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('omw-1.4')\n",
        "\n",
        "# display all df columns (default is 20)\n",
        "pd.options.display.max_columns = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MP847vfIJMN"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/data_usampl_60_40_comments_cleaned_preproc_fasttext.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 360835 entries, 0 to 360834\n",
            "Data columns (total 5 columns):\n",
            " #   Column                 Non-Null Count   Dtype \n",
            "---  ------                 --------------   ----- \n",
            " 0   comment_raw            360835 non-null  object\n",
            " 1   comment_clean          360603 non-null  object\n",
            " 2   comment_clean_preproc  360038 non-null  object\n",
            " 3   ft_vector              360835 non-null  object\n",
            " 4   toxic                  360835 non-null  int64 \n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 13.8+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create label/target variable + basic corpus variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "29jE5PSrPFE2"
      },
      "outputs": [],
      "source": [
        "target = df['toxic']\n",
        "\n",
        "corp_raw = df['comment_raw']\n",
        "corp_clean = df['comment_clean']\n",
        "corp_pp = df['comment_clean_preproc']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train fastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# choose from corp_raw, corp_clean and corp_pp for different fastText\n",
        "# training sets\n",
        "corp2ft = corp_pp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Read 8M words\n",
            "Number of words:  42083\n",
            "Number of labels: 0\n",
            "Progress: 100.0% words/sec/thread:   63546 lr:  0.000000 avg.loss:  2.163434 ETA:   0h 0m 0s\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'float' object has no attribute 'find'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m os\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/fasttext_training_data_tmp.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# get fastText vectors\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m corp_ft \u001b[38;5;241m=\u001b[39m \u001b[43mcorp2ft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sentence_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# convert series of lists to df\u001b[39;00m\n\u001b[1;32m     20\u001b[0m corp_ft \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\\\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;241m.\u001b[39mfrom_dict(\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(corp_ft\u001b[38;5;241m.\u001b[39mindex, corp_ft\u001b[38;5;241m.\u001b[39mvalues)))\u001b[38;5;241m.\u001b[39mT\n",
            "File \u001b[0;32m~/nf-bootcamp/projects/lexyguards/.venv/lib/python3.11/site-packages/pandas/core/series.py:4397\u001b[0m, in \u001b[0;36mSeries.map\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   4318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\n\u001b[1;32m   4319\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4320\u001b[0m     arg: Callable \u001b[38;5;241m|\u001b[39m Mapping \u001b[38;5;241m|\u001b[39m Series,\n\u001b[1;32m   4321\u001b[0m     na_action: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4322\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m   4323\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4324\u001b[0m \u001b[38;5;124;03m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[1;32m   4325\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4395\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[1;32m   4396\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4397\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_values, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[1;32m   4399\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4400\u001b[0m     )\n",
            "File \u001b[0;32m~/nf-bootcamp/projects/lexyguards/.venv/lib/python3.11/site-packages/pandas/core/base.py:924\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# mapper is a function\u001b[39;00m\n\u001b[0;32m--> 924\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mmap_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_values\n",
            "File \u001b[0;32m~/nf-bootcamp/projects/lexyguards/.venv/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/nf-bootcamp/projects/lexyguards/.venv/lib/python3.11/site-packages/fasttext/FastText.py:134\u001b[0m, in \u001b[0;36m_FastText.get_sentence_vector\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_sentence_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Given a string, get a single vector represenation. This function\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    assumes to be given a single line of text. We split words on\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    whitespace (space, newline, tab, vertical tab) and the control\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    characters carriage return, formfeed and the null character.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    136\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict processes one line at a time (remove \u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mn\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         )\n\u001b[1;32m    138\u001b[0m     text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'find'"
          ]
        }
      ],
      "source": [
        "# remove chars that prevent fastText from training\n",
        "#if corp2ft is corp_raw:\n",
        "regex = r'[\\n\\r]'\n",
        "corp_raw = corp_raw.str.replace(regex, ' ', regex=True, case=False)\n",
        "\n",
        "# create temp file for fastText\n",
        "corp2ft.to_csv('data/fasttext_training_data_tmp.csv',\n",
        "               index=False, header=False)\n",
        "\n",
        "# run unsupervised learning to get embeddings\n",
        "ft = fasttext.train_unsupervised('data/fasttext_training_data_tmp.csv')\n",
        "\n",
        "# delete temp file\n",
        "os.remove('data/fasttext_training_data_tmp.csv')\n",
        "\n",
        "# get fastText vectors\n",
        "corp_ft = corp2ft.map(ft.get_sentence_vector)\n",
        "\n",
        "# convert series of lists to df\n",
        "corp_ft = pd.DataFrame\\\n",
        "    .from_dict(dict(zip(corp_ft.index, corp_ft.values))).T\n",
        "\n",
        "corp_ft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train baseline model on raw BOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_bow = CountVectorizer()\n",
        "corp_raw_bow = vect_bow.fit_transform(corp_raw)\n",
        "\n",
        "lr = LogisticRegression(max_iter=2000)\n",
        "lr.fit(corp_raw_bow, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define custom strings to do tox detection on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "custom_strings = pd.Series(np.array([\n",
        "    'I hate gay people!',\n",
        "    'Lesbians suck.',\n",
        "    'You\\'re a pussy and you know it.',\n",
        "    'Fuck you!',\n",
        "    'Up your ass.',\n",
        "    'I really like people.',\n",
        "    'Hello world!',\n",
        "    'Gay men are great!'\n",
        "]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predict: BOW (raw) + baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# vectorize strings\n",
        "custom_strings_bow = vect_bow.transform(custom_strings)\n",
        "\n",
        "# predict strings\n",
        "custom_strings_predict = lr.predict(custom_strings_bow)\n",
        "\n",
        "for i, str in enumerate(custom_strings):\n",
        "    print(i, str, '-->', custom_strings_predict[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predict: fastText (preprocessed) + XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    clean custom_strings\n",
        "    preprocess custom_strings\n",
        "    vectorize custom_strings with fastText\n",
        "    predict custom_strings with XGBoost\n",
        "    output results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean(s):\n",
        "\n",
        "    # remove HTML anchor tags\n",
        "    regex = r'<a .*?>|</a>' # *? for non-greedy repetition\n",
        "    s = re.sub(regex, '', s, flags=re.IGNORECASE)\n",
        "\n",
        "    # remove URLs\n",
        "    regex = r'https?://\\S+'\n",
        "    s = re.sub(regex, '', s, flags=re.IGNORECASE)\n",
        "\n",
        "    # remove newlines (\\n), carriage returns (\\r), unicode line separators (U+2028)\n",
        "    regex = r'[\\n\\r\\u2028]'\n",
        "    s = re.sub(regex, ' ', s, flags=re.IGNORECASE)\n",
        "\n",
        "    # remove numbers and replace with _number_\n",
        "    regex = r'\\d+'\n",
        "    s = re.sub(regex, '_number_', s, flags=re.IGNORECASE)\n",
        "\n",
        "    # \"unmask\" morst frequent swearwords, insults etc. (e.g. f*ck, cr@p)\n",
        "    match_list = '(?i)f*ck, (?i)sh*t, (?i)s**t, (?i)f***, (?i)p***y, (?i)b*tch, (?i)f**k, (?i)p*ssy, (?i)p****, (?i)s***, (?i)a**, (?i)h*ll, (?i)h***, (?i)sh*t, (?i)pu**y, (?i)sh**, (?i)cr*p, (?i)@ss, (?i)cr@p, (?i)b@lls, (?i)f@ck, (?i)waaay, (?i)waaaay, (?i)riiiight, (?i)soo+, (?i)stooooopid, (?i)huu+ge, (?i)yuu+ge, (?i)suu+re'\\\n",
        "        .replace('*', r'\\*').split(', ')\n",
        "    replace_list = 'fuck, shit, shit, fuck, pussy, bitch, fuck, pussy, pussy, shit, ass, hell, hell, shit, pussy, shit, crap, ass, crap, balls, fuck, way, way, right, so, stupid, huge, huge, sure'\\\n",
        "        .split(', ')\n",
        "    for match, repl in zip(match_list, replace_list):\n",
        "        re.sub(match, repl, s, flags=re.IGNORECASE)\n",
        "\n",
        "    ### Remove multiple spaces\n",
        "    regex = r' {2,}'\n",
        "    s = re.sub(regex, ' ', s, flags=re.IGNORECASE)\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "custom_strings_clean = custom_strings.map(clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function that returns list of lemmatized tokens with stop words and\n",
        "# punctuation marks removed\n",
        "def preprocess(s):\n",
        "    doc = nlp(s) # tokenize\n",
        "\n",
        "    final_tokens = []\n",
        "\n",
        "    for token in doc:\n",
        "        if token.is_stop or token.is_punct:\n",
        "           continue # skip punctuation marks and stop words\n",
        "        final_tokens.append(token.lemma_) # lemmatize token\n",
        "\n",
        "    return \" \".join(final_tokens) # convert list to space-separated string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "custom_strings_pp = custom_strings_clean.map(preprocess)\n",
        "print(custom_strings, custom_strings_clean, custom_strings_pp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters for model\n",
        "params = {'random_state': 42,\n",
        "          'n_jobs': -1}\n",
        "\n",
        "# load model with parameters\n",
        "xgb = XGBClassifier(**params)\n",
        "xgb.fit(corp_ft, target)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
